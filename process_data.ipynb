{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import json\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"compilation2.json\", \"r\") as content:\n",
    "    data = json.load(content)['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set types of characters and sequences to be filtered out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "esc = re.compile(\"\\n|u2019|u201c|u201d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['x200b', 'u', 'get', 'im', 'dont', 'go', 'like', 'also', 'real', 'really',\n",
    "             'many', 'even', 'ive', 'way', 'us', 'wont', 'would', 'seem', 'whats', 'ever',\n",
    "             'theyve', 'much', 'however', 'among', 'upon', 'every', 'around', 'could', 'maybe',\n",
    "             'seems', 'anyone', 'anything', 'one', 'else', 'take', 'make', 'see', 'amp',\n",
    "             'something', 'say', 'people', 'know', 'think', 'thing'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns the Lemmatizer tag for the part of speech of a provided word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part_of_speech(word):\n",
    "    tag = pos_tag([word])[0][1][0].lower()\n",
    "    if tag == wordnet.ADJ: \n",
    "        pos = 'j'\n",
    "    elif tag == wordnet.VERB: \n",
    "        pos = 'v'\n",
    "    elif tag == wordnet.ADV:\n",
    "        pos = 'r'\n",
    "    else:\n",
    "        pos = 'n'\n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process text: remove documents with no content, remove undesirable characters, and convert words to their stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    statements = deque()\n",
    "    originals = deque()\n",
    "    for post in data:\n",
    "        if post.get('selftext') and post['selftext'] != \"[removed]\" and post['selftext'] != \"\":\n",
    "            sentence = post['selftext']\n",
    "            \n",
    "            sentence = sentence.lower()  # convert all characters to lower case\n",
    "            sentence = links.sub('', sentence)   # remove  links from text\n",
    "            sentence = esc.sub(' ', sentence)    # remove escape sequences from text\n",
    "            sentence = re.sub(r'[^\\w\\s]',' ', sentence)   # remove non-alphanumeric characters\n",
    "            \n",
    "            words = word_tokenize(sentence)\n",
    "            words = [i for i in words if i not in stop]\n",
    "            words = [lemmatizer.lemmatize(i, part_of_speech(i)) for i in words]\n",
    "            if(len(words) > 3): \n",
    "                statements.append(\" \".join(words))\n",
    "                originals.append(post['selftext'])\n",
    "    return statements, originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*The TV told me outside is dangerous*\\n\\n*The TV told me to stay in my home*\\n\\n*The TV told me human interaction bad*\\n\\n*The TV told me I canâ€™t hang out with you*\\n\\n*The TV told me to call the cops if I see you with your friends*\\n\\n*The TV told me to trust no one*'\n",
      " \"Iv'e linked a .io game from itch.io that's pretty relevant to society as we see it now. It blows my mind to see people have the news tell them how to feel/act. It's a flash game where you control the media and set the narrative for a social downfall.  \\n\\n[https://ncase.itch.io/wbwwb](https://ncase.itch.io/wbwwb)\"\n",
      " 'After all, the protest seemed it had no end in sight?\\n\\nhttps://old.reddit.com/r/AskReddit/comments/fpl9nn/hong_kong_protesters_how_is_it_going_on_the_front/'\n",
      " '\"\"Bill Gates predicted this was going to happen\" NO he KNEW this was going to happen and is about to Holocaust our asses thanks to you fools'\n",
      " 'that witch has ties to so many celebrities probably the handler for all of them looks like she was sprouted from the dept of hell by satan himself']\n",
      "['tv told outside dangerous tv told stay home tv told human interaction bad tv told hang tv told call cop friend tv told trust'\n",
      " 'iv e link io game itch io pretty relevant society blow mind news tell feel act flash game control medium set narrative social downfall'\n",
      " 'protest seem end sight'\n",
      " 'bill gate predict go happen knew go happen holocaust ass thanks fool'\n",
      " 'witch tie celebrity probably handler look sprout dept hell satan']\n"
     ]
    }
   ],
   "source": [
    "statements, originals = process(data)\n",
    "statements = np.array(statements)\n",
    "originals = np.array(originals)\n",
    "print(originals[0:5])\n",
    "print(statements[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total statements:  9065\n",
      "Total words:  40504\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "for stmt in statements:\n",
    "    for word in stmt.split():\n",
    "        words.append(word)\n",
    "words = np.unique(words)\n",
    "            \n",
    "print(\"Total statements: \", len(statements))\n",
    "print(\"Total words: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    return stop\n",
    "\n",
    "def get_processed():\n",
    "    return statements\n",
    "\n",
    "def get_originals():\n",
    "    return originals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
